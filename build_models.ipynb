{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-binance mplfinance\n",
        "!pip install yfinance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npTz1V2ywfD8",
        "outputId": "5e0f5ce5-7dea-45a0-c3c0-bb3ec098b11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-binance\n",
            "  Downloading python_binance-1.0.16-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting mplfinance\n",
            "  Downloading mplfinance-0.12.9b1-py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 9.8 MB/s \n",
            "\u001b[?25hCollecting dateparser\n",
            "  Downloading dateparser-1.1.1-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 44.9 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.2 MB/s \n",
            "\u001b[?25hCollecting ujson\n",
            "  Downloading ujson-5.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from python-binance) (2.23.0)\n",
            "Collecting websockets\n",
            "  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-binance) (1.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mplfinance) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mplfinance) (3.2.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->python-binance) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->python-binance) (4.1.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->python-binance) (21.4.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->python-binance) (2.10)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser->python-binance) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser->python-binance) (2.8.2)\n",
            "Collecting regex!=2019.02.19,!=2021.8.27,<2022.3.15\n",
            "  Downloading regex-2022.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 45.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser->python-binance) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (1.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->python-binance) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->python-binance) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->python-binance) (1.24.3)\n",
            "Installing collected packages: multidict, frozenlist, yarl, regex, asynctest, async-timeout, aiosignal, websockets, ujson, dateparser, aiohttp, python-binance, mplfinance\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 dateparser-1.1.1 frozenlist-1.3.0 mplfinance-0.12.9b1 multidict-6.0.2 python-binance-1.0.16 regex-2022.3.2 ujson-5.3.0 websockets-10.3 yarl-1.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.72-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 27.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.9.0 requests-2.28.0 yfinance-0.1.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZbeAbeFOYO0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime as date\n",
        "import yfinance as yf\n",
        "import binance as bi\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter(item: list):\n",
        "    return item[0:6]\n",
        "\n",
        "\n",
        "class ContextProvider:\n",
        "    def __init__(self) -> None:\n",
        "        self.df = None\n",
        "        self.stock = None\n",
        "        self.start_date = None\n",
        "        self.end_date = None\n",
        "        self.freq = None\n",
        "        self.interval = 1\n",
        "\n",
        "    def get_data(self, stock, start_date, end_date):\n",
        "        if (self.stock == stock and self.start_date == start_date and self.end_date == end_date):\n",
        "            return self.df, self.freq\n",
        "        return self.fetch_data(stock, start_date, end_date)\n",
        "\n",
        "    def fetch_data(self, stock, start_date, end_date):\n",
        "        if stock == \"BTCUSDT\":\n",
        "            # tmp_end = int(date.fromisoformat(end_date).timestamp() * 1000)\n",
        "            client = bi.Client()\n",
        "            res = client.get_klines(\n",
        "                symbol=stock,\n",
        "                interval=client.KLINE_INTERVAL_5MINUTE,\n",
        "                limit=3000,\n",
        "                endTime=int(date.now().timestamp()*1000)\n",
        "            )\n",
        "            fil = list(map(filter, res))\n",
        "            self.df = pd.DataFrame(\n",
        "                fil, columns=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "\n",
        "            self.freq = \"5min\"\n",
        "        else:\n",
        "            yf_ticker_data = yf.Ticker(stock)\n",
        "            self.df = yf_ticker_data.history(\n",
        "                period=\"1d\",\n",
        "                start=date.fromisoformat(start_date).strftime(\"%Y-%m-%d\"),\n",
        "                end=date.fromisoformat(end_date).strftime(\"%Y-%m-%d\"))\n",
        "            self.df = pd.DataFrame(self.df)\n",
        "            self.df = self.df.iloc[:, :-2]\n",
        "            self.df = self.df.reset_index()\n",
        "\n",
        "            self.freq = \"D\"\n",
        "\n",
        "        self.df['Open'] = self.df['Open'].astype('float64')\n",
        "        self.df['High'] = self.df['High'].astype('float64')\n",
        "        self.df['Low'] = self.df['Low'].astype('float64')\n",
        "        self.df['Close'] = self.df['Close'].astype('float64')\n",
        "        self.df['Volume'] = self.df['Volume'].astype('float64')\n",
        "        self.df[\"Date\"] = pd.to_datetime(self.df[\"Date\"], unit='ms')\n",
        "\n",
        "        poc = [100 * (b - a) / a for a,\n",
        "               b in zip(self.df[\"Close\"][::1], self.df[\"Close\"][1::1])]\n",
        "        # the beginning is always set 0\n",
        "        poc.insert(0, 0)\n",
        "        self.df[\"PoC\"] = poc\n",
        "\n",
        "        self.stock = stock\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "\n",
        "        return self.df, self.freq\n",
        "\n",
        "    def handle_ws_message(self, message):\n",
        "        # print(message)\n",
        "        if isinstance(message, str):\n",
        "            m = json.loads(message)\n",
        "            # print(m)\n",
        "            d = {}\n",
        "            diff = 0\n",
        "            at = None\n",
        "            for key, value in m.items():\n",
        "                if key == \"E\":\n",
        "                    event_time = date.fromtimestamp(value/1000)\n",
        "                    diff = (event_time -\n",
        "                            self.df[\"Date\"][len(self.df) - 1]).total_seconds()\n",
        "                    # print(diff)\n",
        "                    at = event_time\n",
        "                if key == \"k\":\n",
        "                    for key2, val2 in value.items():\n",
        "                        if key2 == \"o\":\n",
        "                            d[\"Open\"] = float(val2)\n",
        "                        if key2 == \"c\":\n",
        "                            d[\"Close\"] = float(val2)\n",
        "                        if key2 == \"h\":\n",
        "                            d[\"High\"] = float(val2)\n",
        "                        if key2 == \"l\":\n",
        "                            d[\"Low\"] = float(val2)\n",
        "                        if key2 == \"v\":\n",
        "                            d[\"Volume\"] = float(val2)\n",
        "\n",
        "            # print(new_row)\n",
        "            print(\"adding new candlestick in: -\" + str(60-diff) + \"s\")\n",
        "            if diff > 60:\n",
        "                d[\"Date\"] = at\n",
        "                last = self.df[\"Close\"][len(self.df) - 1]\n",
        "                d[\"PoC\"] = 100*(d[\"Close\"]-last)/last\n",
        "                new_row = pd.DataFrame(d, index=[0])\n",
        "                self.df = pd.concat([self.df, new_row], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "g7lU_T9iwz7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import *\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n"
      ],
      "metadata": {
        "id": "bc1JnGpQw3zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seq_len = 128\n",
        "\n",
        "d_k = 256\n",
        "d_v = 256\n",
        "n_heads = 12\n",
        "ff_dim = 256\n",
        "\n"
      ],
      "metadata": {
        "id": "_vk4St0ow8_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Time2Vector(Layer):\n",
        "    def __init__(self, seq_len, **kwargs):\n",
        "        super(Time2Vector, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # '''Initialize weights and biases with shape (batch, seq_len)'''\n",
        "        self.weights_linear = self.add_weight(name='weight_linear',\n",
        "                                              shape=(int(self.seq_len),),\n",
        "                                              initializer='uniform',\n",
        "                                              trainable=True)\n",
        "\n",
        "        self.bias_linear = self.add_weight(name='bias_linear',\n",
        "                                           shape=(int(self.seq_len),),\n",
        "                                           initializer='uniform',\n",
        "                                           trainable=True)\n",
        "\n",
        "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
        "                                                shape=(int(self.seq_len),),\n",
        "                                                initializer='uniform',\n",
        "                                                trainable=True)\n",
        "\n",
        "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
        "                                             shape=(int(self.seq_len),),\n",
        "                                             initializer='uniform',\n",
        "                                             trainable=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        # '''Calculate linear and periodic time features'''\n",
        "        x = tf.math.reduce_mean(x[:, :, :4], axis=-1)\n",
        "        time_linear = self.weights_linear * x + self.bias_linear  # Linear time feature\n",
        "        # Add dimension (batch, seq_len, 1)\n",
        "        time_linear = tf.expand_dims(time_linear, axis=-1)\n",
        "\n",
        "        time_periodic = tf.math.sin(tf.multiply(\n",
        "            x, self.weights_periodic) + self.bias_periodic)\n",
        "        # Add dimension (batch, seq_len, 1)\n",
        "        time_periodic = tf.expand_dims(time_periodic, axis=-1)\n",
        "        # shape = (batch, seq_len, 2)\n",
        "        return tf.concat([time_linear, time_periodic], axis=-1)\n",
        "\n",
        "    def get_config(self):  # Needed for saving and loading model with custom layer\n",
        "        config = super().get_config().copy()\n",
        "        config.update({'seq_len': self.seq_len})\n",
        "        return config\n",
        "\n",
        "\n",
        "class SingleAttention(Layer):\n",
        "    def __init__(self, d_k, d_v):\n",
        "        super(SingleAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.query = Dense(self.d_k,\n",
        "                           input_shape=input_shape,\n",
        "                           kernel_initializer='glorot_uniform',\n",
        "                           bias_initializer='glorot_uniform')\n",
        "\n",
        "        self.key = Dense(self.d_k,\n",
        "                         input_shape=input_shape,\n",
        "                         kernel_initializer='glorot_uniform',\n",
        "                         bias_initializer='glorot_uniform')\n",
        "\n",
        "        self.value = Dense(self.d_v,\n",
        "                           input_shape=input_shape,\n",
        "                           kernel_initializer='glorot_uniform',\n",
        "                           bias_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, inputs):  # inputs = (in_seq, in_seq, in_seq)\n",
        "        q = self.query(inputs[0])\n",
        "        k = self.key(inputs[1])\n",
        "\n",
        "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
        "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
        "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
        "\n",
        "        v = self.value(inputs[2])\n",
        "        attn_out = tf.matmul(attn_weights, v)\n",
        "        return attn_out\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "\n",
        "class MultiAttention(Layer):\n",
        "    def __init__(self, d_k, d_v, n_heads):\n",
        "        super(MultiAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.n_heads = n_heads\n",
        "        self.attn_heads = list()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        for n in range(self.n_heads):\n",
        "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))\n",
        "\n",
        "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7\n",
        "        self.linear = Dense(input_shape[0][-1],\n",
        "                            input_shape=input_shape,\n",
        "                            kernel_initializer='glorot_uniform',\n",
        "                            bias_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
        "        concat_attn = tf.concat(attn, axis=-1)\n",
        "        multi_linear = self.linear(concat_attn)\n",
        "        return multi_linear\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "\n",
        "class TransformerEncoder(Layer):\n",
        "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.n_heads = n_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.attn_heads = list()\n",
        "        self.dropout_rate = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
        "        self.attn_dropout = Dropout(self.dropout_rate)\n",
        "        self.attn_normalize = LayerNormalization(\n",
        "            input_shape=input_shape, epsilon=1e-6)\n",
        "\n",
        "        self.ff_conv1D_1 = Conv1D(\n",
        "            filters=self.ff_dim, kernel_size=1, activation='relu')\n",
        "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7\n",
        "        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1)\n",
        "        self.ff_dropout = Dropout(self.dropout_rate)\n",
        "        self.ff_normalize = LayerNormalization(\n",
        "            input_shape=input_shape, epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):  # inputs = (in_seq, in_seq, in_seq)\n",
        "        attn_layer = self.attn_multi(inputs)\n",
        "        attn_layer = self.attn_dropout(attn_layer)\n",
        "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
        "\n",
        "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
        "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
        "        ff_layer = self.ff_dropout(ff_layer)\n",
        "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
        "        return ff_layer\n",
        "\n",
        "    def get_config(self):  # Needed for saving and loading model with custom layer\n",
        "        config = super().get_config().copy()\n",
        "        config.update({'d_k': self.d_k,\n",
        "                       'd_v': self.d_v,\n",
        "                       'n_heads': self.n_heads,\n",
        "                       'ff_dim': self.ff_dim,\n",
        "                       'attn_heads': self.attn_heads,\n",
        "                       'dropout_rate': self.dropout_rate})\n",
        "        return config\n",
        "\n",
        "\n",
        "def create_model(dense=1):\n",
        "    # '''Initialize time and transformer layers'''\n",
        "    time_embedding = Time2Vector(seq_len)\n",
        "    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "    attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "\n",
        "    # '''Construct model'''\n",
        "    in_seq = Input(shape=(seq_len, 5))\n",
        "    x = time_embedding(in_seq)\n",
        "    x = Concatenate(axis=-1)([in_seq, x])\n",
        "    x = attn_layer1((x, x, x))\n",
        "    x = attn_layer2((x, x, x))\n",
        "    x = attn_layer3((x, x, x))\n",
        "    x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    out = Dense(dense, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=in_seq, outputs=out)\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def TATE_train_model(X_train, y_train, X_val, y_val, n_forecast, stock):\n",
        "    model = create_model(n_forecast)\n",
        "    model.summary()\n",
        "\n",
        "    callback = tf.keras.callbacks.ModelCheckpoint(f'models/{stock}_Transformer_TimeEmbedding.h5',\n",
        "                                                  monitor='val_loss',\n",
        "                                                  verbose=0)\n",
        "\n",
        "    model.fit(X_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=20,\n",
        "              callbacks=[callback],\n",
        "              validation_data=(X_val, y_val))\n",
        "\n",
        "    model = tf.keras.models.load_model(f'models/{stock}_Transformer_TimeEmbedding.h5',\n",
        "                                       custom_objects={'Time2Vector': Time2Vector,\n",
        "                                                       'SingleAttention': SingleAttention,\n",
        "                                                       'MultiAttention': MultiAttention,\n",
        "                                                       'TransformerEncoder': TransformerEncoder})\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def TATE_load_model(stock):\n",
        "    model = tf.keras.models.load_model(f'models/{stock}_Transformer_TimeEmbedding.h5',\n",
        "                                       custom_objects={'Time2Vector': Time2Vector,\n",
        "                                                       'SingleAttention': SingleAttention,\n",
        "                                                       'MultiAttention': MultiAttention,\n",
        "                                                       'TransformerEncoder': TransformerEncoder})\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ZkqSl1M6w_oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "from keras.models import Sequential\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n"
      ],
      "metadata": {
        "id": "ABW2577JxCI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_build(units, x_train, y_train, dense, stock, feature):\n",
        "    f = f'models/{stock}_{feature}_LSTM.h5'\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(LSTM(units=units, return_sequences=True,\n",
        "                        input_shape=(x_train.shape[1], 1)))\n",
        "    lstm_model.add(LSTM(units=units))\n",
        "    lstm_model.add(Dense(dense))\n",
        "\n",
        "    lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    lstm_model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "    lstm_model.save(f)\n",
        "\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "def RNN_build(units, dropout, x_train, y_train, dense, stock, feature):\n",
        "    f = f'models/{stock}_{feature}_RNN.h5'\n",
        "    rnn_model = Sequential()\n",
        "    rnn_model.add(LSTM(units=units, return_sequences=True,\n",
        "                       input_shape=(x_train.shape[1], 1)))\n",
        "    rnn_model.add(Dropout(dropout))\n",
        "    for i in [True, True, False]:\n",
        "        rnn_model.add(LSTM(units=units, return_sequences=i))\n",
        "        rnn_model.add(Dropout(dropout))\n",
        "\n",
        "    rnn_model.add(Dense(units=dense))\n",
        "    rnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    rnn_model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "    rnn_model.save(f)\n",
        "\n",
        "    return rnn_model\n",
        "\n",
        "\n",
        "def XGBoost_build(X_train, y_train, eval_set, stock, feature):\n",
        "    f = f'models/{stock}_{feature}_XGBoost.pkl'\n",
        "    start_time = time()\n",
        "    model = XGBRegressor(max_depth=7)\n",
        "    model.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
        "    pickle.dump(model, open(f, \"wb\"))\n",
        "    print('Fit time : ', time() - start_time)\n",
        "    return model\n",
        "\n",
        "\n",
        "def LSTM_train_forecast_prices(df, n_lookback, n_forecast, feature, dt_freq, stock):\n",
        "    length = len(df)\n",
        "    data = df\n",
        "\n",
        "    new_dataset = pd.DataFrame(index=range(\n",
        "        0, length), columns=['Date', feature])\n",
        "\n",
        "    for i in range(0, len(data)):\n",
        "        new_dataset[\"Date\"][i] = data['Date'][i]\n",
        "        new_dataset[feature][i] = data[feature][i]\n",
        "\n",
        "    new_dataset.index = new_dataset.Date\n",
        "    new_dataset.drop(\"Date\", axis=1, inplace=True)\n",
        "    final_dataset = new_dataset.values\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(final_dataset)\n",
        "\n",
        "    x_train_data, y_train_data = [], []\n",
        "    for i in range(n_lookback, len(final_dataset) - n_forecast + 1):\n",
        "        x_train_data.append(scaled_data[i-n_lookback:i, 0])\n",
        "        y_train_data.append(scaled_data[i:i+n_forecast, 0])\n",
        "\n",
        "    x_train_data, y_train_data = np.array(x_train_data), np.array(y_train_data)\n",
        "\n",
        "    x_train_data = np.reshape(\n",
        "        x_train_data, (x_train_data.shape[0], x_train_data.shape[1]))\n",
        "\n",
        "    X_test = scaled_data[-n_lookback:]\n",
        "    X_test = np.array(X_test).reshape(1, n_lookback)\n",
        "\n",
        "    _model = LSTM_build(\n",
        "        50, x_train_data, y_train_data, n_forecast, stock, feature)\n",
        "\n",
        "    predicted_closing_price = _model.predict(X_test)\n",
        "    predicted_closing_price = scaler.inverse_transform(predicted_closing_price)\n",
        "\n",
        "    df_past = new_dataset[:]\n",
        "    t = pd.date_range(\n",
        "        start=data['Date'][len(data)-1], periods=n_forecast, freq=dt_freq)\n",
        "    df_future = pd.DataFrame(columns=[\"Date\", \"Predictions\"])\n",
        "    df_future[\"Date\"] = t\n",
        "    df_future.index = df_future[\"Date\"]\n",
        "    df_future.drop(\"Date\", axis=1, inplace=True)\n",
        "    df_future[\"Predictions\"] = predicted_closing_price.flatten()\n",
        "\n",
        "    return df_past, df_future\n",
        "\n",
        "\n",
        "def RNN_train_forecast_prices(df, n_lookback, n_forecast, feature, dt_freq, stock):\n",
        "    length = len(df)\n",
        "    data = df\n",
        "\n",
        "    new_dataset = pd.DataFrame(index=range(\n",
        "        0, length), columns=['Date', feature])\n",
        "\n",
        "    for i in range(0, len(data)):\n",
        "        new_dataset[\"Date\"][i] = data['Date'][i]\n",
        "        new_dataset[feature][i] = data[feature][i]\n",
        "\n",
        "    new_dataset.index = new_dataset.Date\n",
        "    new_dataset.drop(\"Date\", axis=1, inplace=True)\n",
        "    final_dataset = new_dataset.values\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(final_dataset)\n",
        "\n",
        "    x_train_data, y_train_data = [], []\n",
        "    for i in range(n_lookback, len(final_dataset) - n_forecast + 1):\n",
        "        x_train_data.append(scaled_data[i-n_lookback:i, 0])\n",
        "        y_train_data.append(scaled_data[i:i+n_forecast, 0])\n",
        "\n",
        "    x_train_data, y_train_data = np.array(x_train_data), np.array(y_train_data)\n",
        "\n",
        "    x_train_data = np.reshape(\n",
        "        x_train_data, (x_train_data.shape[0], x_train_data.shape[1]))\n",
        "\n",
        "    X_test = scaled_data[-n_lookback:]\n",
        "    X_test = np.array(X_test).reshape(1, n_lookback)\n",
        "\n",
        "    _model = RNN_build(\n",
        "        45, 0.2, x_train_data, y_train_data, n_forecast, stock, feature)\n",
        "\n",
        "    predicted_closing_price = _model.predict(X_test)\n",
        "    predicted_closing_price = scaler.inverse_transform(predicted_closing_price)\n",
        "\n",
        "    df_past = new_dataset[:]\n",
        "    t = pd.date_range(\n",
        "        start=data['Date'][len(data)-1], periods=n_forecast, freq=dt_freq)\n",
        "    df_future = pd.DataFrame(columns=[\"Date\", \"Predictions\"])\n",
        "    df_future[\"Date\"] = t\n",
        "    df_future.index = df_future[\"Date\"]\n",
        "    df_future.drop(\"Date\", axis=1, inplace=True)\n",
        "    df_future[\"Predictions\"] = predicted_closing_price.flatten()\n",
        "\n",
        "    return df_past, df_future\n",
        "\n",
        "\n",
        "def XGBoost_train_forecast_prices(df, n_lookback, n_forecast, feature, dt_freq, stock):\n",
        "    length = len(df)\n",
        "    data = df\n",
        "\n",
        "    new_dataset = pd.DataFrame(index=range(\n",
        "        0, length), columns=['Date', feature])\n",
        "\n",
        "    for i in range(0, len(data)):\n",
        "        new_dataset[\"Date\"][i] = data['Date'][i]\n",
        "        new_dataset[feature][i] = data[feature][i]\n",
        "\n",
        "    new_dataset.index = new_dataset.Date\n",
        "    new_dataset.drop(\"Date\", axis=1, inplace=True)\n",
        "    final_dataset = new_dataset.values\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(final_dataset)\n",
        "\n",
        "    X_test = scaled_data[-n_lookback:]\n",
        "    X_test = np.array(X_test).reshape(1, n_lookback)\n",
        "\n",
        "    test_size = 0.1\n",
        "    test_ind = int(len(final_dataset) * (1-test_size))\n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(n_lookback, test_ind - n_forecast + 1):\n",
        "        x_train.append(scaled_data[i-n_lookback:i, 0])\n",
        "        y_train.append(scaled_data[i:i+n_forecast, 0])\n",
        "\n",
        "    x_train, y_train = np.array(\n",
        "        x_train), np.array(y_train)\n",
        "\n",
        "    x_train = np.reshape(\n",
        "        x_train, (x_train.shape[0], x_train.shape[1]))\n",
        "\n",
        "    x_valid, y_valid = [], []\n",
        "    for i in range(test_ind-n_forecast+1, len(final_dataset) - n_forecast + 1):\n",
        "        x_valid.append(scaled_data[i-n_lookback:i, 0])\n",
        "        y_valid.append(scaled_data[i:i+n_forecast, 0])\n",
        "\n",
        "    x_valid, y_valid = np.array(x_valid), np.array(y_valid)\n",
        "    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1])\n",
        "\n",
        "    _model = XGBoost_build(x_train, y_train, [\n",
        "        (x_train, y_train), (x_valid, y_valid)], stock, feature)\n",
        "\n",
        "    predicted_closing_price = _model.predict(X_test)\n",
        "    predicted_closing_price = scaler.inverse_transform(predicted_closing_price)\n",
        "\n",
        "    df_past = new_dataset[:]\n",
        "    t = pd.date_range(\n",
        "        start=data['Date'][len(data)-1], periods=n_forecast, freq=dt_freq)\n",
        "    df_future = pd.DataFrame(columns=[\"Date\", \"Predictions\"])\n",
        "    df_future[\"Date\"] = t\n",
        "    df_future.index = df_future[\"Date\"]\n",
        "    df_future.drop(\"Date\", axis=1, inplace=True)\n",
        "    df_future[\"Predictions\"] = predicted_closing_price.flatten()\n",
        "\n",
        "    return df_past, df_future\n",
        "\n",
        "\n",
        "def TATE_train_forecast_prices(df, n_forecast, dt_freq, feature, stock):\n",
        "    __df = df\n",
        "    cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    df = df[cols].copy()\n",
        "\n",
        "    # '''Calculate percentage change'''\n",
        "\n",
        "    df['Open'] = df['Open'].pct_change()  # Create arithmetic returns column\n",
        "    df['High'] = df['High'].pct_change()  # Create arithmetic returns column\n",
        "    df['Low'] = df['Low'].pct_change()  # Create arithmetic returns column\n",
        "    df['Close'] = df['Close'].pct_change()  # Create arithmetic returns column\n",
        "    df['Volume'] = df['Volume'].pct_change()\n",
        "\n",
        "    df.dropna(how='any', axis=0, inplace=True)  # Drop all rows with NaN values\n",
        "\n",
        "    ###############################################################################\n",
        "    # '''Create indexes to split dataset'''\n",
        "\n",
        "    times = sorted(df.index.values)\n",
        "    # Last 20% of series\n",
        "    last_20pct = sorted(df.index.values)[-int(0.2*len(times))]\n",
        "\n",
        "    ###############################################################################\n",
        "    # '''Normalize price columns'''\n",
        "\n",
        "    min_return = min(df[(df.index < last_20pct)]\n",
        "                     [['Open', 'High', 'Low', 'Close']].min(axis=0))\n",
        "    max_return = max(df[(df.index < last_20pct)]\n",
        "                     [['Open', 'High', 'Low', 'Close']].max(axis=0))\n",
        "\n",
        "    # Min-max normalize price columns (0-1 range)\n",
        "    df['Open'] = (df['Open'] - min_return) / (max_return - min_return)\n",
        "    df['High'] = (df['High'] - min_return) / (max_return - min_return)\n",
        "    df['Low'] = (df['Low'] - min_return) / (max_return - min_return)\n",
        "    df['Close'] = (df['Close'] - min_return) / (max_return - min_return)\n",
        "\n",
        "    ###############################################################################\n",
        "    # '''Normalize volume column'''\n",
        "\n",
        "    min_volume = df[(df.index < last_20pct)]['Volume'].min(axis=0)\n",
        "    max_volume = df[(df.index < last_20pct)]['Volume'].max(axis=0)\n",
        "\n",
        "    # Min-max normalize volume columns (0-1 range)\n",
        "    df['Volume'] = (df['Volume'] - min_volume) / (max_volume - min_volume)\n",
        "\n",
        "    ###############################################################################\n",
        "    # '''Create training, validation and test split'''\n",
        "\n",
        "    # Training data are 80% of total data\n",
        "    df_train = df[(df.index < last_20pct - n_forecast)].copy()\n",
        "    df_val = df[(df.index >= last_20pct - n_forecast)].copy()\n",
        "\n",
        "    # Remove date column\n",
        "    df_train.drop(columns=['Date'], axis=1, inplace=True)\n",
        "    df_val.drop(columns=['Date'], axis=1, inplace=True)\n",
        "\n",
        "    # scaled data\n",
        "    scaled_data = df['Close'].values\n",
        "\n",
        "    # Convert pandas columns into arrays\n",
        "    train_data = df_train.values\n",
        "    val_data = df_val.values\n",
        "\n",
        "    # Training data\n",
        "    X_train, y_train = [], []\n",
        "    for i in range(seq_len, len(train_data)):\n",
        "        # Chunks of training data with a length of 128 df-rows\n",
        "        X_train.append(train_data[i-seq_len:i])\n",
        "        # Value of the feature that we work on\n",
        "        y_train.append(scaled_data[i:i+n_forecast])\n",
        "    X_train, y_train = np.array(\n",
        "        X_train, dtype=list), np.array(y_train, dtype=list)\n",
        "    X_train = X_train.reshape(X_train.shape[0], seq_len, X_train.shape[2])\n",
        "    y_train = y_train.reshape(y_train.shape[0], n_forecast)\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    y_train = y_train.astype(np.float32)\n",
        "\n",
        "    ###############################################################################\n",
        "\n",
        "    # Validation data\n",
        "    X_val, y_val = [], []\n",
        "    for i in range(seq_len, len(val_data) - n_forecast + 1):\n",
        "        X_val.append(val_data[i-seq_len:i])\n",
        "        y_val.append(scaled_data[i:i+n_forecast])\n",
        "    X_val, y_val = np.array(X_val, dtype=list), np.array(y_val, dtype=list)\n",
        "    X_val = X_val.reshape(X_val.shape[0], seq_len, X_val.shape[2])\n",
        "    y_val = y_val.reshape(y_val.shape[0], n_forecast)\n",
        "    X_val = X_val.astype(np.float32)\n",
        "    y_val = y_val.astype(np.float32)\n",
        "\n",
        "    ###############################################################################\n",
        "\n",
        "    # Test data\n",
        "    X_test = val_data[-seq_len:]\n",
        "    X_test = np.array(X_test, dtype=list)\n",
        "    X_test = X_test.reshape(1, X_test.shape[0], X_test.shape[1])\n",
        "    X_test = X_test.astype(np.float32)\n",
        "    # print(X_train.shape)\n",
        "    # print(y_train.shape)\n",
        "    # print(X_test.shape)\n",
        "\n",
        "    _model = TATE_train_model(X_train, y_train, X_val,\n",
        "                              y_val, n_forecast, stock)\n",
        "\n",
        "    test_pred = _model.predict(X_test)\n",
        "    # print(test_pred)\n",
        "    pred = test_pred.flatten()*(max_return - min_return) + min_return\n",
        "    # print(pred)\n",
        "    pred = pd.DataFrame(pred, columns=['Predictions'])\n",
        "    pred = pred['Predictions'].add(1, fill_value=0).cumprod()*__df['Close'][len(__df)-1]\n",
        "    # print(pred.values)\n",
        "\n",
        "    df_past = __df[:]\n",
        "    t = pd.date_range(\n",
        "        start=__df['Date'][len(__df)-1], periods=n_forecast, freq=dt_freq)\n",
        "    df_future = pd.DataFrame(columns=[\"Date\", \"Predictions\"])\n",
        "    df_future[\"Date\"] = t\n",
        "    df_future.index = df_future[\"Date\"]\n",
        "    df_future.drop(\"Date\", axis=1, inplace=True)\n",
        "    df_future[\"Predictions\"] = pred.values\n",
        "    # print(df_future)\n",
        "\n",
        "    return df_past, df_future\n",
        "\n"
      ],
      "metadata": {
        "id": "gX1XgHIdxIPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from keras.models import load_model\n"
      ],
      "metadata": {
        "id": "4M8ZoXaL1vg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def LSTM_load(stock, feature):\n",
        "    f = f'models/{stock}_{feature}_RNN.h5'\n",
        "    if path.exists(f):\n",
        "        lstm_model = load_model(f)\n",
        "\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "def RNN_load(stock, feature):\n",
        "    f = f'models/{stock}_{feature}_RNN.h5'\n",
        "    if path.exists(f):\n",
        "        rnn_model = load_model(f)\n",
        "\n",
        "    return rnn_model\n",
        "\n",
        "\n",
        "def XGBoost_load(stock, feature):\n",
        "    f = f'models/{stock}_{feature}_XGBoost.pkl'\n",
        "    if path.exists(f):\n",
        "        model = pickle.load(open(f, \"rb\"))\n",
        "    return model\n",
        "\n",
        "\n",
        "def LSTM_load_forecast_prices(df, n_lookback, n_forecast, feature, dt_freq, stock):\n",
        "    length = len(df)\n",
        "    data = df\n",
        "\n",
        "    new_dataset = pd.DataFrame(index=range(\n",
        "        0, length), columns=['Date', feature])\n",
        "\n",
        "    for i in range(0, len(data)):\n",
        "        new_dataset[\"Date\"][i] = data['Date'][i]\n",
        "        new_dataset[feature][i] = data[feature][i]\n",
        "\n",
        "    new_dataset.index = new_dataset.Date\n",
        "    new_dataset.drop(\"Date\", axis=1, inplace=True)\n",
        "    final_dataset = new_dataset.values\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(final_dataset)\n",
        "\n",
        "    X_test = scaled_data[-n_lookback:]\n",
        "    X_test = np.array(X_test).reshape(1, n_lookback)\n",
        "\n",
        "    _model = LSTM_load(stock, feature)\n",
        "\n",
        "    predicted_closing_price = _model.predict(X_test)\n",
        "    predicted_closing_price = scaler.inverse_transform(predicted_closing_price)\n",
        "\n",
        "    df_past = new_dataset[:]\n",
        "    t = pd.date_range(\n",
        "        start=data['Date'][len(data)-1], periods=n_forecast, freq=dt_freq)\n",
        "    df_future = pd.DataFrame(columns=[\"Date\", \"Predictions\"])\n",
        "    df_future[\"Date\"] = t\n",
        "    df_future.index = df_future[\"Date\"]\n",
        "    df_future.drop(\"Date\", axis=1, inplace=True)\n",
        "    df_future[\"Predictions\"] = predicted_closing_price.flatten()\n",
        "\n",
        "    return df_past, df_future\n",
        "\n",
        "\n",
        "def RNN_load_forecast_prices(df, n_lookback, n_forecast, feature, dt_freq, stock):\n",
        "    length = len(df)\n",
        "    data = df\n",
        "\n",
        "    new_dataset = pd.DataFrame(index=range(\n",
        "        0, length), columns=['Date', feature])\n",
        "\n",
        "    for i in range(0, len(data)):\n",
        "        new_dataset[\"Date\"][i] = data['Date'][i]\n",
        "        new_dataset[feature][i] = data[feature][i]\n",
        "\n",
        "    new_dataset.index = new_dataset.Date\n",
        "    new_dataset.drop(\"Date\", axis=1, inplace=True)\n",
        "    final_dataset = new_dataset.values\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(final_dataset)\n",
        "\n",
        "    X_test = scaled_data[-n_lookback:]\n",
        "    X_test = np.array(X_test).reshape(1, n_lookback)\n",
        "\n",
        "    _model = RNN_load(stock, feature)\n",
        "\n",
        "    predicted_closing_price = _model.predict(X_test)\n",
        "    predicted_closing_price = scaler.inverse_transform(predicted_closing_price)\n",
        "\n",
        "    df_past = new_dataset[:]\n",
        "    t = pd.date_range(\n",
        "        start=data['Date'][len(data)-1], periods=n_forecast, freq=dt_freq)\n",
        "    df_future = pd.DataFrame(columns=[\"Date\", \"Predictions\"])\n",
        "    df_future[\"Date\"] = t\n",
        "    df_future.index = df_future[\"Date\"]\n",
        "    df_future.drop(\"Date\", axis=1, inplace=True)\n",
        "    df_future[\"Predictions\"] = predicted_closing_price.flatten()\n",
        "\n",
        "    return df_past, df_future\n",
        "\n",
        "\n",
        "def XGBoost_load_forecast_prices(df, n_lookback, n_forecast, feature, dt_freq, stock):\n",
        "    length = len(df)\n",
        "    data = df\n",
        "\n",
        "    new_dataset = pd.DataFrame(index=range(\n",
        "        0, length), columns=['Date', feature])\n",
        "\n",
        "    for i in range(0, len(data)):\n",
        "        new_dataset[\"Date\"][i] = data['Date'][i]\n",
        "        new_dataset[feature][i] = data[feature][i]\n",
        "\n",
        "    new_dataset.index = new_dataset.Date\n",
        "    new_dataset.drop(\"Date\", axis=1, inplace=True)\n",
        "    final_dataset = new_dataset.values\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(final_dataset)\n",
        "\n",
        "    X_test = scaled_data[-n_lookback:]\n",
        "    X_test = np.array(X_test).reshape(1, n_lookback)\n",
        "\n",
        "    _model = XGBoost_load(stock, feature)\n",
        "\n",
        "    predicted_closing_price = _model.predict(X_test)\n",
        "    predicted_closing_price = scaler.inverse_transform(predicted_closing_price)\n",
        "\n",
        "    df_past = new_dataset[:]\n",
        "    t = pd.date_range(\n",
        "        start=data['Date'][len(data)-1], periods=n_forecast, freq=dt_freq)\n",
        "    df_future = pd.DataFrame(columns=[\"Date\", \"Predictions\"])\n",
        "    df_future[\"Date\"] = t\n",
        "    df_future.index = df_future[\"Date\"]\n",
        "    df_future.drop(\"Date\", axis=1, inplace=True)\n",
        "    df_future[\"Predictions\"] = predicted_closing_price.flatten()\n",
        "\n",
        "    return df_past, df_future\n",
        "\n",
        "\n",
        "def TATE_load_forecast_prices(df, n_forecast, dt_freq, feature, stock):\n",
        "    __df = df\n",
        "    cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    df = df[cols].copy()\n",
        "\n",
        "    # '''Calculate percentage change'''\n",
        "\n",
        "    df['Open'] = df['Open'].pct_change()  # Create arithmetic returns column\n",
        "    df['High'] = df['High'].pct_change()  # Create arithmetic returns column\n",
        "    df['Low'] = df['Low'].pct_change()  # Create arithmetic returns column\n",
        "    df['Close'] = df['Close'].pct_change()  # Create arithmetic returns column\n",
        "    df['Volume'] = df['Volume'].pct_change()\n",
        "\n",
        "    df.dropna(how='any', axis=0, inplace=True)  # Drop all rows with NaN values\n",
        "\n",
        "    ###############################################################################\n",
        "    # '''Create indexes to split dataset'''\n",
        "\n",
        "    times = sorted(df.index.values)\n",
        "    # Last 20% of series\n",
        "    last_20pct = sorted(df.index.values)[-int(0.2*len(times))]\n",
        "\n",
        "    ###############################################################################\n",
        "    # '''Normalize price columns'''\n",
        "\n",
        "    min_return = min(df[(df.index < last_20pct)]\n",
        "                     [['Open', 'High', 'Low', 'Close']].min(axis=0))\n",
        "    max_return = max(df[(df.index < last_20pct)]\n",
        "                     [['Open', 'High', 'Low', 'Close']].max(axis=0))\n",
        "\n",
        "    # Min-max normalize price columns (0-1 range)\n",
        "    df['Open'] = (df['Open'] - min_return) / (max_return - min_return)\n",
        "    df['High'] = (df['High'] - min_return) / (max_return - min_return)\n",
        "    df['Low'] = (df['Low'] - min_return) / (max_return - min_return)\n",
        "    df['Close'] = (df['Close'] - min_return) / (max_return - min_return)\n",
        "\n",
        "    ###############################################################################\n",
        "    # '''Normalize volume column'''\n",
        "\n",
        "    min_volume = df[(df.index < last_20pct)]['Volume'].min(axis=0)\n",
        "    max_volume = df[(df.index < last_20pct)]['Volume'].max(axis=0)\n",
        "\n",
        "    # Min-max normalize volume columns (0-1 range)\n",
        "    df['Volume'] = (df['Volume'] - min_volume) / (max_volume - min_volume)\n",
        "\n",
        "    ###############################################################################\n",
        "    # '''Create training, validation and test split'''\n",
        "\n",
        "    # Training data are 80% of total data\n",
        "    df_val = df[(df.index >= last_20pct - n_forecast)].copy()\n",
        "\n",
        "    # Remove date column\n",
        "    df_val.drop(columns=['Date'], axis=1, inplace=True)\n",
        "\n",
        "    # Convert pandas columns into arrays\n",
        "    val_data = df_val.values\n",
        "\n",
        "    ###############################################################################\n",
        "\n",
        "    # Test data\n",
        "    X_test = val_data[-seq_len:]\n",
        "    X_test = np.array(X_test, dtype=list)\n",
        "    X_test = X_test.reshape(1, X_test.shape[0], X_test.shape[1])\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "    _model = TATE_load_model(stock)\n",
        "\n",
        "    test_pred = _model.predict(X_test)\n",
        "    # print(test_pred)\n",
        "    pred = test_pred.flatten()*(max_return - min_return) + min_return\n",
        "    # print(pred)\n",
        "    pred = pd.DataFrame(pred, columns=['Predictions'])  \n",
        "    pred = pred['Predictions'].add(1, fill_value=0).cumprod()*__df['Close'][len(__df)-1]\n",
        "    # print(pred.values)\n",
        "\n",
        "    df_past = __df[:]\n",
        "    t = pd.date_range(\n",
        "        start=__df['Date'][len(__df)-1], periods=n_forecast, freq=dt_freq)\n",
        "    df_future = pd.DataFrame(columns=[\"Date\", \"Predictions\"])\n",
        "    df_future[\"Date\"] = t\n",
        "    df_future.index = df_future[\"Date\"]\n",
        "    df_future.drop(\"Date\", axis=1, inplace=True)\n",
        "    df_future[\"Predictions\"] = pred.values\n",
        "    # print(df_future)\n",
        "\n",
        "    return df_past, df_future\n"
      ],
      "metadata": {
        "id": "Wrs5b4BM1gmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "24g3JIXDxT45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with gpu on colab much faster than on my local pc\n",
        "\n",
        "1 epoch = 44s while it's 150s on my pc (3 times faster)"
      ],
      "metadata": {
        "id": "KOzTmcig9XLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks = ['AAPL', 'BTCUSDT', 'META', 'GOOG', 'NFLX', 'TSLA']\n",
        "# stocks = ['AAPL']\n",
        "# features = [\"Close\", \"PoC\"]\n",
        "features = [\"Close\"]\n",
        "# models = [\"RNN\", \"LSTM\", \"TATE\"]\n",
        "models = [\"TATE\"]\n",
        "\n",
        "ctx = ContextProvider()\n",
        "\n",
        "for s in stocks:\n",
        "    df, freq = ctx.get_data(s, datetime(2010, 1, 1).isoformat(), datetime.now().isoformat())\n",
        "    # print(df)\n",
        "    for f in features:\n",
        "        for m in models:\n",
        "            # if m == \"XGBoost\":\n",
        "            #     XGBoost_train_forecast_prices(df, 60, 15, f, freq, s)\n",
        "            # if m == \"RNN\":\n",
        "            #     RNN_train_forecast_prices(df, 60, 15, f, freq, s)\n",
        "            # if m == \"LSTM\":\n",
        "            #     LSTM_train_forecast_prices(df, 60, 15, f, freq, s)\n",
        "            if m == \"TATE\":\n",
        "                TATE_train_forecast_prices(df, 15, freq, f, s)\n",
        "            # print(p)\n",
        "            # print(f)\n",
        "        \n"
      ],
      "metadata": {
        "id": "F_OqsG8NxLXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42dec1c1-419a-4ec4-debe-06957e056b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
            "                                                                                                  \n",
            " time2_vector_2 (Time2Vector)   (None, 128, 2)       512         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 128, 7)       0           ['input_2[0][0]',                \n",
            "                                                                  'time2_vector_2[0][0]']         \n",
            "                                                                                                  \n",
            " transformer_encoder_6 (Transfo  (None, 128, 7)      99114       ['concatenate_1[0][0]',          \n",
            " rmerEncoder)                                                     'concatenate_1[0][0]',          \n",
            "                                                                  'concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " transformer_encoder_7 (Transfo  (None, 128, 7)      99114       ['transformer_encoder_6[0][0]',  \n",
            " rmerEncoder)                                                     'transformer_encoder_6[0][0]',  \n",
            "                                                                  'transformer_encoder_6[0][0]']  \n",
            "                                                                                                  \n",
            " transformer_encoder_8 (Transfo  (None, 128, 7)      99114       ['transformer_encoder_7[0][0]',  \n",
            " rmerEncoder)                                                     'transformer_encoder_7[0][0]',  \n",
            "                                                                  'transformer_encoder_7[0][0]']  \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 128)         0           ['transformer_encoder_8[0][0]']  \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 128)          0           ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           8256        ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 64)           0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           975         ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 307,085\n",
            "Trainable params: 307,085\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "75/75 [==============================] - 75s 651ms/step - loss: 0.0789 - mae: 0.2068 - mape: 38.3262 - val_loss: 0.0020 - val_mae: 0.0331 - val_mape: 6.1286\n",
            "Epoch 2/20\n",
            "75/75 [==============================] - 45s 595ms/step - loss: 0.0068 - mae: 0.0647 - mape: 12.1189 - val_loss: 0.0022 - val_mae: 0.0350 - val_mape: 6.3354\n",
            "Epoch 3/20\n",
            "75/75 [==============================] - 45s 598ms/step - loss: 0.0058 - mae: 0.0597 - mape: 11.1846 - val_loss: 0.0022 - val_mae: 0.0355 - val_mape: 6.4149\n",
            "Epoch 4/20\n",
            "75/75 [==============================] - 44s 582ms/step - loss: 0.0052 - mae: 0.0561 - mape: 10.5320 - val_loss: 0.0021 - val_mae: 0.0343 - val_mape: 6.2353\n",
            "Epoch 5/20\n",
            "75/75 [==============================] - 44s 582ms/step - loss: 0.0048 - mae: 0.0539 - mape: 10.1181 - val_loss: 0.0022 - val_mae: 0.0354 - val_mape: 6.7476\n",
            "Epoch 6/20\n",
            "75/75 [==============================] - 44s 585ms/step - loss: 0.0046 - mae: 0.0528 - mape: 9.9197 - val_loss: 0.0021 - val_mae: 0.0340 - val_mape: 6.1893\n",
            "Epoch 7/20\n",
            "75/75 [==============================] - 44s 583ms/step - loss: 0.0042 - mae: 0.0504 - mape: 9.4756 - val_loss: 0.0025 - val_mae: 0.0385 - val_mape: 6.9062\n",
            "Epoch 8/20\n",
            "75/75 [==============================] - 44s 581ms/step - loss: 0.0040 - mae: 0.0493 - mape: 9.2840 - val_loss: 0.0030 - val_mae: 0.0434 - val_mape: 7.7381\n",
            "Epoch 9/20\n",
            "75/75 [==============================] - 43s 581ms/step - loss: 0.0039 - mae: 0.0484 - mape: 9.1091 - val_loss: 0.0019 - val_mae: 0.0328 - val_mape: 6.1669\n",
            "Epoch 10/20\n",
            "75/75 [==============================] - 43s 580ms/step - loss: 0.0036 - mae: 0.0467 - mape: 8.7912 - val_loss: 0.0024 - val_mae: 0.0374 - val_mape: 6.7203\n",
            "Epoch 11/20\n",
            "75/75 [==============================] - 44s 583ms/step - loss: 0.0036 - mae: 0.0465 - mape: 8.7445 - val_loss: 0.0020 - val_mae: 0.0327 - val_mape: 6.0277\n",
            "Epoch 12/20\n",
            "75/75 [==============================] - 43s 580ms/step - loss: 0.0035 - mae: 0.0458 - mape: 8.6128 - val_loss: 0.0019 - val_mae: 0.0325 - val_mape: 6.0436\n",
            "Epoch 13/20\n",
            "75/75 [==============================] - 44s 594ms/step - loss: 0.0035 - mae: 0.0455 - mape: 8.5624 - val_loss: 0.0024 - val_mae: 0.0368 - val_mape: 6.6203\n",
            "Epoch 14/20\n",
            "75/75 [==============================] - 43s 578ms/step - loss: 0.0033 - mae: 0.0442 - mape: 8.3245 - val_loss: 0.0020 - val_mae: 0.0330 - val_mape: 6.0568\n",
            "Epoch 15/20\n",
            "75/75 [==============================] - 43s 575ms/step - loss: 0.0033 - mae: 0.0441 - mape: 8.3041 - val_loss: 0.0020 - val_mae: 0.0327 - val_mape: 6.0221\n",
            "Epoch 16/20\n",
            "75/75 [==============================] - 43s 577ms/step - loss: 0.0032 - mae: 0.0430 - mape: 8.1151 - val_loss: 0.0024 - val_mae: 0.0373 - val_mape: 6.6937\n",
            "Epoch 17/20\n",
            "75/75 [==============================] - 43s 575ms/step - loss: 0.0033 - mae: 0.0439 - mape: 8.2721 - val_loss: 0.0020 - val_mae: 0.0334 - val_mape: 6.1039\n",
            "Epoch 18/20\n",
            "75/75 [==============================] - 43s 576ms/step - loss: 0.0031 - mae: 0.0421 - mape: 7.9504 - val_loss: 0.0019 - val_mae: 0.0326 - val_mape: 6.0174\n",
            "Epoch 19/20\n",
            "75/75 [==============================] - 44s 585ms/step - loss: 0.0030 - mae: 0.0420 - mape: 7.9227 - val_loss: 0.0020 - val_mae: 0.0332 - val_mape: 6.0801\n",
            "Epoch 20/20\n",
            "75/75 [==============================] - 43s 576ms/step - loss: 0.0029 - mae: 0.0412 - mape: 7.7729 - val_loss: 0.0023 - val_mae: 0.0362 - val_mape: 6.5221\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
            "                                                                                                  \n",
            " time2_vector_4 (Time2Vector)   (None, 128, 2)       512         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 128, 7)       0           ['input_3[0][0]',                \n",
            "                                                                  'time2_vector_4[0][0]']         \n",
            "                                                                                                  \n",
            " transformer_encoder_12 (Transf  (None, 128, 7)      99114       ['concatenate_2[0][0]',          \n",
            " ormerEncoder)                                                    'concatenate_2[0][0]',          \n",
            "                                                                  'concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " transformer_encoder_13 (Transf  (None, 128, 7)      99114       ['transformer_encoder_12[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_12[0][0]', \n",
            "                                                                  'transformer_encoder_12[0][0]'] \n",
            "                                                                                                  \n",
            " transformer_encoder_14 (Transf  (None, 128, 7)      99114       ['transformer_encoder_13[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_13[0][0]', \n",
            "                                                                  'transformer_encoder_13[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 128)         0           ['transformer_encoder_14[0][0]'] \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128)          0           ['global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 64)           8256        ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 64)           0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 15)           975         ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 307,085\n",
            "Trainable params: 307,085\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "21/21 [==============================] - 40s 802ms/step - loss: 0.2458 - mae: 0.4796 - mape: 85.2968 - val_loss: 0.1030 - val_mae: 0.2832 - val_mape: 49.5937\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - 12s 571ms/step - loss: 0.0413 - mae: 0.1610 - mape: 29.5586 - val_loss: 0.0169 - val_mae: 0.1017 - val_mape: 22.1834\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - 12s 573ms/step - loss: 0.0139 - mae: 0.0909 - mape: 17.5874 - val_loss: 0.0151 - val_mae: 0.0951 - val_mape: 19.9741\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - 12s 567ms/step - loss: 0.0133 - mae: 0.0889 - mape: 17.1922 - val_loss: 0.0147 - val_mae: 0.0939 - val_mape: 20.0852\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - 12s 575ms/step - loss: 0.0124 - mae: 0.0858 - mape: 16.6193 - val_loss: 0.0147 - val_mae: 0.0935 - val_mape: 19.8330\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - 12s 570ms/step - loss: 0.0124 - mae: 0.0853 - mape: 16.5166 - val_loss: 0.0151 - val_mae: 0.0941 - val_mape: 19.4187\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - 12s 573ms/step - loss: 0.0120 - mae: 0.0839 - mape: 16.3234 - val_loss: 0.0147 - val_mae: 0.0940 - val_mape: 20.1806\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - 12s 571ms/step - loss: 0.0116 - mae: 0.0825 - mape: 16.0633 - val_loss: 0.0147 - val_mae: 0.0939 - val_mape: 20.1285\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - 12s 570ms/step - loss: 0.0116 - mae: 0.0819 - mape: 15.9854 - val_loss: 0.0147 - val_mae: 0.0938 - val_mape: 20.0569\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - 12s 577ms/step - loss: 0.0112 - mae: 0.0813 - mape: 15.8464 - val_loss: 0.0149 - val_mae: 0.0956 - val_mape: 20.9138\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - 12s 572ms/step - loss: 0.0113 - mae: 0.0812 - mape: 15.8361 - val_loss: 0.0148 - val_mae: 0.0935 - val_mape: 19.7009\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - 12s 571ms/step - loss: 0.0110 - mae: 0.0802 - mape: 15.6416 - val_loss: 0.0151 - val_mae: 0.0940 - val_mape: 19.4085\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - 12s 571ms/step - loss: 0.0110 - mae: 0.0801 - mape: 15.5788 - val_loss: 0.0147 - val_mae: 0.0940 - val_mape: 20.2311\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - 12s 576ms/step - loss: 0.0107 - mae: 0.0786 - mape: 15.4407 - val_loss: 0.0149 - val_mae: 0.0936 - val_mape: 19.4934\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - 12s 578ms/step - loss: 0.0108 - mae: 0.0793 - mape: 15.4922 - val_loss: 0.0149 - val_mae: 0.0936 - val_mape: 19.4984\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - 13s 616ms/step - loss: 0.0108 - mae: 0.0791 - mape: 15.4695 - val_loss: 0.0147 - val_mae: 0.0939 - val_mape: 20.1586\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - 12s 570ms/step - loss: 0.0104 - mae: 0.0771 - mape: 15.0850 - val_loss: 0.0147 - val_mae: 0.0935 - val_mape: 19.8266\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - 12s 575ms/step - loss: 0.0105 - mae: 0.0779 - mape: 15.2814 - val_loss: 0.0147 - val_mae: 0.0936 - val_mape: 19.9387\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - 12s 567ms/step - loss: 0.0107 - mae: 0.0784 - mape: 15.3212 - val_loss: 0.0147 - val_mae: 0.0938 - val_mape: 20.1212\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - 12s 576ms/step - loss: 0.0105 - mae: 0.0781 - mape: 15.2653 - val_loss: 0.0148 - val_mae: 0.0948 - val_mape: 20.6047\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
            "                                                                                                  \n",
            " time2_vector_6 (Time2Vector)   (None, 128, 2)       512         ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 128, 7)       0           ['input_4[0][0]',                \n",
            "                                                                  'time2_vector_6[0][0]']         \n",
            "                                                                                                  \n",
            " transformer_encoder_18 (Transf  (None, 128, 7)      99114       ['concatenate_3[0][0]',          \n",
            " ormerEncoder)                                                    'concatenate_3[0][0]',          \n",
            "                                                                  'concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " transformer_encoder_19 (Transf  (None, 128, 7)      99114       ['transformer_encoder_18[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_18[0][0]', \n",
            "                                                                  'transformer_encoder_18[0][0]'] \n",
            "                                                                                                  \n",
            " transformer_encoder_20 (Transf  (None, 128, 7)      99114       ['transformer_encoder_19[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_19[0][0]', \n",
            "                                                                  'transformer_encoder_19[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d_3 (Gl  (None, 128)         0           ['transformer_encoder_20[0][0]'] \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 128)          0           ['global_average_pooling1d_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 64)           8256        ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 64)           0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 15)           975         ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 307,085\n",
            "Trainable params: 307,085\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "60/60 [==============================] - 64s 665ms/step - loss: 0.0590 - mae: 0.1990 - mape: 111971.3203 - val_loss: 0.0038 - val_mae: 0.0430 - val_mape: 11.0296\n",
            "Epoch 2/20\n",
            "60/60 [==============================] - 35s 586ms/step - loss: 0.0052 - mae: 0.0551 - mape: 190663.5625 - val_loss: 0.0035 - val_mae: 0.0399 - val_mape: 10.1198\n",
            "Epoch 3/20\n",
            "60/60 [==============================] - 35s 581ms/step - loss: 0.0044 - mae: 0.0503 - mape: 202292.8594 - val_loss: 0.0036 - val_mae: 0.0403 - val_mape: 10.1776\n",
            "Epoch 4/20\n",
            "60/60 [==============================] - 35s 577ms/step - loss: 0.0039 - mae: 0.0472 - mape: 184183.7500 - val_loss: 0.0033 - val_mae: 0.0385 - val_mape: 10.0671\n",
            "Epoch 5/20\n",
            "60/60 [==============================] - 35s 582ms/step - loss: 0.0037 - mae: 0.0455 - mape: 188247.2969 - val_loss: 0.0034 - val_mae: 0.0393 - val_mape: 10.0404\n",
            "Epoch 6/20\n",
            "60/60 [==============================] - 36s 601ms/step - loss: 0.0034 - mae: 0.0439 - mape: 192535.5469 - val_loss: 0.0033 - val_mae: 0.0385 - val_mape: 10.0610\n",
            "Epoch 7/20\n",
            "60/60 [==============================] - 35s 579ms/step - loss: 0.0033 - mae: 0.0426 - mape: 196890.4844 - val_loss: 0.0033 - val_mae: 0.0388 - val_mape: 9.9845\n",
            "Epoch 8/20\n",
            "60/60 [==============================] - 35s 581ms/step - loss: 0.0032 - mae: 0.0416 - mape: 201791.2031 - val_loss: 0.0035 - val_mae: 0.0397 - val_mape: 10.0804\n",
            "Epoch 9/20\n",
            "60/60 [==============================] - 35s 584ms/step - loss: 0.0031 - mae: 0.0408 - mape: 201574.4531 - val_loss: 0.0034 - val_mae: 0.0388 - val_mape: 9.9944\n",
            "Epoch 10/20\n",
            "60/60 [==============================] - 35s 588ms/step - loss: 0.0029 - mae: 0.0395 - mape: 202004.3906 - val_loss: 0.0035 - val_mae: 0.0395 - val_mape: 10.0648\n",
            "Epoch 11/20\n",
            "60/60 [==============================] - 36s 594ms/step - loss: 0.0028 - mae: 0.0387 - mape: 197013.5938 - val_loss: 0.0032 - val_mae: 0.0386 - val_mape: 10.1807\n",
            "Epoch 12/20\n",
            "60/60 [==============================] - 35s 590ms/step - loss: 0.0028 - mae: 0.0385 - mape: 188145.4219 - val_loss: 0.0033 - val_mae: 0.0385 - val_mape: 10.0303\n",
            "Epoch 13/20\n",
            "60/60 [==============================] - 35s 589ms/step - loss: 0.0027 - mae: 0.0382 - mape: 193620.8438 - val_loss: 0.0033 - val_mae: 0.0387 - val_mape: 9.9877\n",
            "Epoch 14/20\n",
            "60/60 [==============================] - 37s 610ms/step - loss: 0.0027 - mae: 0.0375 - mape: 194523.9375 - val_loss: 0.0034 - val_mae: 0.0394 - val_mape: 10.0405\n",
            "Epoch 15/20\n",
            "60/60 [==============================] - 35s 584ms/step - loss: 0.0026 - mae: 0.0366 - mape: 192968.2344 - val_loss: 0.0033 - val_mae: 0.0385 - val_mape: 10.0072\n",
            "Epoch 16/20\n",
            "60/60 [==============================] - 35s 592ms/step - loss: 0.0026 - mae: 0.0371 - mape: 192155.9062 - val_loss: 0.0034 - val_mae: 0.0390 - val_mape: 10.0030\n",
            "Epoch 17/20\n",
            "60/60 [==============================] - 36s 594ms/step - loss: 0.0025 - mae: 0.0362 - mape: 203332.4531 - val_loss: 0.0033 - val_mae: 0.0385 - val_mape: 10.0267\n",
            "Epoch 18/20\n",
            "60/60 [==============================] - 35s 589ms/step - loss: 0.0025 - mae: 0.0357 - mape: 198778.6562 - val_loss: 0.0033 - val_mae: 0.0385 - val_mape: 10.0247\n",
            "Epoch 19/20\n",
            "60/60 [==============================] - 35s 592ms/step - loss: 0.0025 - mae: 0.0361 - mape: 196890.6094 - val_loss: 0.0033 - val_mae: 0.0386 - val_mape: 9.9832\n",
            "Epoch 20/20\n",
            "60/60 [==============================] - 35s 585ms/step - loss: 0.0025 - mae: 0.0359 - mape: 202198.2344 - val_loss: 0.0033 - val_mae: 0.0390 - val_mape: 10.4157\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
            "                                                                                                  \n",
            " time2_vector_8 (Time2Vector)   (None, 128, 2)       512         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 128, 7)       0           ['input_5[0][0]',                \n",
            "                                                                  'time2_vector_8[0][0]']         \n",
            "                                                                                                  \n",
            " transformer_encoder_24 (Transf  (None, 128, 7)      99114       ['concatenate_4[0][0]',          \n",
            " ormerEncoder)                                                    'concatenate_4[0][0]',          \n",
            "                                                                  'concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " transformer_encoder_25 (Transf  (None, 128, 7)      99114       ['transformer_encoder_24[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_24[0][0]', \n",
            "                                                                  'transformer_encoder_24[0][0]'] \n",
            "                                                                                                  \n",
            " transformer_encoder_26 (Transf  (None, 128, 7)      99114       ['transformer_encoder_25[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_25[0][0]', \n",
            "                                                                  'transformer_encoder_25[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d_4 (Gl  (None, 128)         0           ['transformer_encoder_26[0][0]'] \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 128)          0           ['global_average_pooling1d_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 64)           8256        ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 64)           0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 15)           975         ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 307,085\n",
            "Trainable params: 307,085\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "75/75 [==============================] - 72s 643ms/step - loss: 0.0477 - mae: 0.1629 - mape: 41.5784 - val_loss: 0.0044 - val_mae: 0.0449 - val_mape: 13.5669\n",
            "Epoch 2/20\n",
            "75/75 [==============================] - 44s 585ms/step - loss: 0.0069 - mae: 0.0624 - mape: 16.5728 - val_loss: 0.0043 - val_mae: 0.0442 - val_mape: 13.1034\n",
            "Epoch 3/20\n",
            "75/75 [==============================] - 44s 584ms/step - loss: 0.0058 - mae: 0.0567 - mape: 15.1820 - val_loss: 0.0043 - val_mae: 0.0450 - val_mape: 12.9809\n",
            "Epoch 4/20\n",
            "75/75 [==============================] - 43s 580ms/step - loss: 0.0054 - mae: 0.0547 - mape: 14.6726 - val_loss: 0.0043 - val_mae: 0.0448 - val_mape: 12.9726\n",
            "Epoch 5/20\n",
            "75/75 [==============================] - 43s 579ms/step - loss: 0.0051 - mae: 0.0526 - mape: 14.1640 - val_loss: 0.0043 - val_mae: 0.0449 - val_mape: 12.9730\n",
            "Epoch 6/20\n",
            "75/75 [==============================] - 44s 583ms/step - loss: 0.0048 - mae: 0.0505 - mape: 13.6123 - val_loss: 0.0043 - val_mae: 0.0441 - val_mape: 12.9616\n",
            "Epoch 7/20\n",
            "75/75 [==============================] - 45s 595ms/step - loss: 0.0046 - mae: 0.0497 - mape: 13.4244 - val_loss: 0.0043 - val_mae: 0.0440 - val_mape: 13.0290\n",
            "Epoch 8/20\n",
            "75/75 [==============================] - 43s 578ms/step - loss: 0.0045 - mae: 0.0489 - mape: 13.2430 - val_loss: 0.0045 - val_mae: 0.0460 - val_mape: 14.0560\n",
            "Epoch 9/20\n",
            "75/75 [==============================] - 44s 581ms/step - loss: 0.0046 - mae: 0.0491 - mape: 13.3023 - val_loss: 0.0043 - val_mae: 0.0443 - val_mape: 13.3049\n",
            "Epoch 10/20\n",
            "75/75 [==============================] - 43s 578ms/step - loss: 0.0043 - mae: 0.0473 - mape: 12.8201 - val_loss: 0.0043 - val_mae: 0.0442 - val_mape: 13.2151\n",
            "Epoch 11/20\n",
            "75/75 [==============================] - 44s 581ms/step - loss: 0.0043 - mae: 0.0472 - mape: 12.8258 - val_loss: 0.0046 - val_mae: 0.0466 - val_mape: 14.2791\n",
            "Epoch 12/20\n",
            "75/75 [==============================] - 43s 580ms/step - loss: 0.0042 - mae: 0.0467 - mape: 12.6844 - val_loss: 0.0043 - val_mae: 0.0446 - val_mape: 12.9395\n",
            "Epoch 13/20\n",
            "75/75 [==============================] - 44s 592ms/step - loss: 0.0041 - mae: 0.0456 - mape: 12.4076 - val_loss: 0.0043 - val_mae: 0.0440 - val_mape: 13.0282\n",
            "Epoch 14/20\n",
            "75/75 [==============================] - 44s 584ms/step - loss: 0.0041 - mae: 0.0457 - mape: 12.4618 - val_loss: 0.0043 - val_mae: 0.0440 - val_mape: 12.9790\n",
            "Epoch 15/20\n",
            "75/75 [==============================] - 43s 575ms/step - loss: 0.0040 - mae: 0.0451 - mape: 12.2795 - val_loss: 0.0044 - val_mae: 0.0454 - val_mape: 13.0052\n",
            "Epoch 16/20\n",
            "75/75 [==============================] - 43s 577ms/step - loss: 0.0041 - mae: 0.0454 - mape: 12.3464 - val_loss: 0.0043 - val_mae: 0.0440 - val_mape: 13.0036\n",
            "Epoch 17/20\n",
            "75/75 [==============================] - 44s 584ms/step - loss: 0.0039 - mae: 0.0443 - mape: 12.1063 - val_loss: 0.0043 - val_mae: 0.0442 - val_mape: 13.2477\n",
            "Epoch 18/20\n",
            "75/75 [==============================] - 44s 587ms/step - loss: 0.0039 - mae: 0.0443 - mape: 12.0862 - val_loss: 0.0045 - val_mae: 0.0455 - val_mape: 13.8473\n",
            "Epoch 19/20\n",
            "75/75 [==============================] - 44s 581ms/step - loss: 0.0039 - mae: 0.0443 - mape: 12.1111 - val_loss: 0.0043 - val_mae: 0.0444 - val_mape: 12.9261\n",
            "Epoch 20/20\n",
            "75/75 [==============================] - 45s 598ms/step - loss: 0.0038 - mae: 0.0436 - mape: 11.9306 - val_loss: 0.0043 - val_mae: 0.0448 - val_mape: 12.9579\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5245b6be60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
            "                                                                                                  \n",
            " time2_vector_10 (Time2Vector)  (None, 128, 2)       512         ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 128, 7)       0           ['input_6[0][0]',                \n",
            "                                                                  'time2_vector_10[0][0]']        \n",
            "                                                                                                  \n",
            " transformer_encoder_30 (Transf  (None, 128, 7)      99114       ['concatenate_5[0][0]',          \n",
            " ormerEncoder)                                                    'concatenate_5[0][0]',          \n",
            "                                                                  'concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " transformer_encoder_31 (Transf  (None, 128, 7)      99114       ['transformer_encoder_30[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_30[0][0]', \n",
            "                                                                  'transformer_encoder_30[0][0]'] \n",
            "                                                                                                  \n",
            " transformer_encoder_32 (Transf  (None, 128, 7)      99114       ['transformer_encoder_31[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_31[0][0]', \n",
            "                                                                  'transformer_encoder_31[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d_5 (Gl  (None, 128)         0           ['transformer_encoder_32[0][0]'] \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 128)          0           ['global_average_pooling1d_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 64)           8256        ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 64)           0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 15)           975         ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 307,085\n",
            "Trainable params: 307,085\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "75/75 [==============================] - 73s 656ms/step - loss: 0.0447 - mae: 0.1522 - mape: 35.3756 - val_loss: 0.0026 - val_mae: 0.0353 - val_mape: 10.7828\n",
            "Epoch 2/20\n",
            "75/75 [==============================] - 45s 600ms/step - loss: 0.0048 - mae: 0.0527 - mape: 12.7701 - val_loss: 0.0022 - val_mae: 0.0311 - val_mape: 10.1744\n",
            "Epoch 3/20\n",
            "75/75 [==============================] - 45s 598ms/step - loss: 0.0041 - mae: 0.0487 - mape: 11.8186 - val_loss: 0.0022 - val_mae: 0.0311 - val_mape: 10.2331\n",
            "Epoch 4/20\n",
            "75/75 [==============================] - 45s 599ms/step - loss: 0.0036 - mae: 0.0457 - mape: 11.1602 - val_loss: 0.0022 - val_mae: 0.0312 - val_mape: 10.2846\n",
            "Epoch 5/20\n",
            "75/75 [==============================] - 46s 609ms/step - loss: 0.0033 - mae: 0.0433 - mape: 10.6049 - val_loss: 0.0024 - val_mae: 0.0336 - val_mape: 10.4587\n",
            "Epoch 6/20\n",
            "75/75 [==============================] - 45s 597ms/step - loss: 0.0032 - mae: 0.0422 - mape: 10.3803 - val_loss: 0.0022 - val_mae: 0.0316 - val_mape: 10.4541\n",
            "Epoch 7/20\n",
            "75/75 [==============================] - 45s 596ms/step - loss: 0.0029 - mae: 0.0402 - mape: 9.9252 - val_loss: 0.0022 - val_mae: 0.0311 - val_mape: 10.2683\n",
            "Epoch 8/20\n",
            "75/75 [==============================] - 44s 592ms/step - loss: 0.0028 - mae: 0.0394 - mape: 9.7245 - val_loss: 0.0023 - val_mae: 0.0325 - val_mape: 10.2918\n",
            "Epoch 9/20\n",
            "75/75 [==============================] - 44s 590ms/step - loss: 0.0027 - mae: 0.0383 - mape: 9.4836 - val_loss: 0.0023 - val_mae: 0.0334 - val_mape: 11.0481\n",
            "Epoch 10/20\n",
            "75/75 [==============================] - 45s 595ms/step - loss: 0.0026 - mae: 0.0375 - mape: 9.2799 - val_loss: 0.0022 - val_mae: 0.0313 - val_mape: 10.3505\n",
            "Epoch 11/20\n",
            "75/75 [==============================] - 45s 597ms/step - loss: 0.0025 - mae: 0.0363 - mape: 9.0002 - val_loss: 0.0023 - val_mae: 0.0326 - val_mape: 10.3058\n",
            "Epoch 12/20\n",
            "75/75 [==============================] - 46s 612ms/step - loss: 0.0025 - mae: 0.0365 - mape: 9.0434 - val_loss: 0.0022 - val_mae: 0.0311 - val_mape: 10.1548\n",
            "Epoch 13/20\n",
            "75/75 [==============================] - 45s 600ms/step - loss: 0.0024 - mae: 0.0357 - mape: 8.8914 - val_loss: 0.0023 - val_mae: 0.0323 - val_mape: 10.2592\n",
            "Epoch 14/20\n",
            "75/75 [==============================] - 45s 596ms/step - loss: 0.0024 - mae: 0.0351 - mape: 8.7233 - val_loss: 0.0022 - val_mae: 0.0317 - val_mape: 10.5205\n",
            "Epoch 15/20\n",
            "75/75 [==============================] - 45s 596ms/step - loss: 0.0023 - mae: 0.0345 - mape: 8.6059 - val_loss: 0.0023 - val_mae: 0.0330 - val_mape: 10.9166\n",
            "Epoch 16/20\n",
            "75/75 [==============================] - 44s 592ms/step - loss: 0.0022 - mae: 0.0341 - mape: 8.5150 - val_loss: 0.0023 - val_mae: 0.0322 - val_mape: 10.2451\n",
            "Epoch 17/20\n",
            "75/75 [==============================] - 45s 598ms/step - loss: 0.0022 - mae: 0.0336 - mape: 8.4044 - val_loss: 0.0026 - val_mae: 0.0370 - val_mape: 12.0759\n",
            "Epoch 18/20\n",
            "75/75 [==============================] - 45s 607ms/step - loss: 0.0022 - mae: 0.0338 - mape: 8.4632 - val_loss: 0.0022 - val_mae: 0.0313 - val_mape: 10.1517\n",
            "Epoch 19/20\n",
            "75/75 [==============================] - 46s 601ms/step - loss: 0.0021 - mae: 0.0324 - mape: 8.1235 - val_loss: 0.0022 - val_mae: 0.0316 - val_mape: 10.1722\n",
            "Epoch 20/20\n",
            "75/75 [==============================] - 45s 600ms/step - loss: 0.0021 - mae: 0.0322 - mape: 8.0733 - val_loss: 0.0022 - val_mae: 0.0310 - val_mape: 10.2206\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f52459de830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 128, 5)]     0           []                               \n",
            "                                                                                                  \n",
            " time2_vector_12 (Time2Vector)  (None, 128, 2)       512         ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 128, 7)       0           ['input_7[0][0]',                \n",
            "                                                                  'time2_vector_12[0][0]']        \n",
            "                                                                                                  \n",
            " transformer_encoder_36 (Transf  (None, 128, 7)      99114       ['concatenate_6[0][0]',          \n",
            " ormerEncoder)                                                    'concatenate_6[0][0]',          \n",
            "                                                                  'concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " transformer_encoder_37 (Transf  (None, 128, 7)      99114       ['transformer_encoder_36[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_36[0][0]', \n",
            "                                                                  'transformer_encoder_36[0][0]'] \n",
            "                                                                                                  \n",
            " transformer_encoder_38 (Transf  (None, 128, 7)      99114       ['transformer_encoder_37[0][0]', \n",
            " ormerEncoder)                                                    'transformer_encoder_37[0][0]', \n",
            "                                                                  'transformer_encoder_37[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d_6 (Gl  (None, 128)         0           ['transformer_encoder_38[0][0]'] \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 128)          0           ['global_average_pooling1d_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 64)           8256        ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 64)           0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 15)           975         ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 307,085\n",
            "Trainable params: 307,085\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "71/71 [==============================] - 65s 666ms/step - loss: 0.0311 - mae: 0.1304 - mape: 121617.2812 - val_loss: 0.0037 - val_mae: 0.0434 - val_mape: 731194.0625\n",
            "Epoch 2/20\n",
            "71/71 [==============================] - 42s 593ms/step - loss: 0.0056 - mae: 0.0560 - mape: 153791.7031 - val_loss: 0.0038 - val_mae: 0.0437 - val_mape: 719468.3750\n",
            "Epoch 3/20\n",
            "71/71 [==============================] - 42s 596ms/step - loss: 0.0050 - mae: 0.0528 - mape: 156624.0625 - val_loss: 0.0045 - val_mae: 0.0498 - val_mape: 679680.8750\n",
            "Epoch 4/20\n",
            "71/71 [==============================] - 42s 598ms/step - loss: 0.0048 - mae: 0.0512 - mape: 152250.0469 - val_loss: 0.0039 - val_mae: 0.0444 - val_mape: 711919.1875\n",
            "Epoch 5/20\n",
            "71/71 [==============================] - 43s 609ms/step - loss: 0.0046 - mae: 0.0499 - mape: 159243.0156 - val_loss: 0.0037 - val_mae: 0.0434 - val_mape: 728328.3125\n",
            "Epoch 6/20\n",
            "71/71 [==============================] - 42s 595ms/step - loss: 0.0045 - mae: 0.0497 - mape: 154407.8125 - val_loss: 0.0037 - val_mae: 0.0433 - val_mape: 734263.3750\n",
            "Epoch 7/20\n",
            "71/71 [==============================] - 42s 594ms/step - loss: 0.0043 - mae: 0.0480 - mape: 158140.7188 - val_loss: 0.0037 - val_mae: 0.0434 - val_mape: 727755.6250\n",
            "Epoch 8/20\n",
            "71/71 [==============================] - 42s 596ms/step - loss: 0.0042 - mae: 0.0476 - mape: 152587.6875 - val_loss: 0.0037 - val_mae: 0.0433 - val_mape: 739488.1875\n",
            "Epoch 9/20\n",
            "71/71 [==============================] - 42s 594ms/step - loss: 0.0041 - mae: 0.0467 - mape: 155504.2188 - val_loss: 0.0037 - val_mae: 0.0433 - val_mape: 737087.8125\n",
            "Epoch 10/20\n",
            "71/71 [==============================] - 42s 598ms/step - loss: 0.0040 - mae: 0.0462 - mape: 150260.7031 - val_loss: 0.0037 - val_mae: 0.0439 - val_mape: 752555.0625\n",
            "Epoch 11/20\n",
            "71/71 [==============================] - 42s 597ms/step - loss: 0.0040 - mae: 0.0460 - mape: 153059.8594 - val_loss: 0.0037 - val_mae: 0.0434 - val_mape: 726581.6250\n",
            "Epoch 12/20\n",
            "71/71 [==============================] - 43s 612ms/step - loss: 0.0039 - mae: 0.0453 - mape: 160600.6719 - val_loss: 0.0037 - val_mae: 0.0433 - val_mape: 735429.5000\n",
            "Epoch 13/20\n",
            "71/71 [==============================] - 43s 601ms/step - loss: 0.0039 - mae: 0.0450 - mape: 153782.7344 - val_loss: 0.0040 - val_mae: 0.0449 - val_mape: 707094.5000\n",
            "Epoch 14/20\n",
            "71/71 [==============================] - 42s 598ms/step - loss: 0.0039 - mae: 0.0447 - mape: 152957.8750 - val_loss: 0.0037 - val_mae: 0.0433 - val_mape: 735386.3125\n",
            "Epoch 15/20\n",
            "71/71 [==============================] - 42s 595ms/step - loss: 0.0038 - mae: 0.0445 - mape: 156255.7344 - val_loss: 0.0037 - val_mae: 0.0435 - val_mape: 744941.4375\n",
            "Epoch 16/20\n",
            "71/71 [==============================] - 42s 589ms/step - loss: 0.0038 - mae: 0.0441 - mape: 157455.1875 - val_loss: 0.0037 - val_mae: 0.0433 - val_mape: 733413.4375\n",
            "Epoch 17/20\n",
            "71/71 [==============================] - 42s 591ms/step - loss: 0.0038 - mae: 0.0440 - mape: 150629.0000 - val_loss: 0.0037 - val_mae: 0.0433 - val_mape: 739137.1250\n",
            "Epoch 18/20\n",
            "71/71 [==============================] - 42s 589ms/step - loss: 0.0038 - mae: 0.0439 - mape: 151185.7344 - val_loss: 0.0037 - val_mae: 0.0439 - val_mape: 753090.5000\n",
            "Epoch 19/20\n",
            "71/71 [==============================] - 43s 607ms/step - loss: 0.0037 - mae: 0.0437 - mape: 154118.5781 - val_loss: 0.0037 - val_mae: 0.0435 - val_mape: 745614.5625\n",
            "Epoch 20/20\n",
            "71/71 [==============================] - 42s 595ms/step - loss: 0.0037 - mae: 0.0434 - mape: 158993.4062 - val_loss: 0.0037 - val_mae: 0.0434 - val_mape: 726905.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yS_jGqyI_Wpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ca56d6-9426-4eff-fe56-d7c423554e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}